# DSPy vs LangGraph vs CrewAI: Same Pipeline, Three Philosophies

A practitioner's comparison of three AI agent frameworks â€” not from docs, but from building the **same production pipeline** in all three.

**The pipeline:** A Company Research Agent that searches the web, extracts structured facts, writes an analyst summary, reviews it with an LLM judge, and loops back if quality isn't good enough.

## Pipeline Architecture
```
Input: "Apple"
  â”‚
  â–¼
web_search(mock) â”€â”€â–¶ Researcher â”€â”€â–¶ Writer â”€â”€â–¶ structural_check()
                     CompanyFacts    Summary         â”‚
                                                fail â”‚ pass
                                              â”Œâ”€â”€â”€â”€â”€â”€â”¤
                                              â”‚      â–¼
                                              â”‚   Reviewer (LLM judge)
                                              â”‚      â”‚
                                              â”‚  approved? â”€â”€yesâ”€â”€â–¶ Output
                                              â”‚      â”‚ no
                                              â”‚      â–¼
                                              â””â”€â”€ Rewriter (with feedback)
                                                  (max 3 iterations)
```

**Two-stage evaluation:** `structural_check()` catches obvious failures (word count, missing fields) for free â€” the LLM judge only runs when structural checks pass. The judge verifies claims against sources, checks facet coverage, and rates conciseness. No vague 0-10 scores.

**What makes this different:**
- **MCP Integration** â€” each framework connected to real MCP servers, including [mcp-python-repl](https://pypi.org/project/mcp-python-repl/)
- **Agent Skills** â€” structured agent capabilities using the [Agent Skills](https://agentskills.io) spec, with scripts, references, and multi-file navigation
- **LLM-as-a-Judge** â€” decomposed evaluation: claim verification, facet coverage, source-grounded accuracy
- **DSPy GEPA optimization** â€” can prompt optimization fix the skill trigger reliability problem [Vercel identified](https://vercel.com/blog/agents-md-outperforms-skills-in-our-agent-evals)?

## Blog Series

| Part | Title | Status |
|------|-------|--------|
| 1 | [Same Pipeline, Three Philosophies](https://faunaris.ai/blog/dspy-langgraph-crewai-part1) | âœ… Published |
| 2 | Building the Same Pipeline, Three Ways | ðŸš§ In progress |
| 3 | MCP, Agent Skills & Dependency Audit | â¬œ Planned |
| 4 | Evaluation, Optimization & Verdict | â¬œ Planned |

## Project Structure
```
â”œâ”€â”€ common/                  # Shared across all frameworks
â”‚   â”œâ”€â”€ models.py            # Pydantic models (CompanyFacts, AnalystSummary, ReviewResult)
â”‚   â”œâ”€â”€ tools.py             # Web search (mock â†’ MCP in Part 3)
â”‚   â””â”€â”€ skills/              # Agent Skills (SKILL.md + scripts + references)
â”‚       â””â”€â”€ company-researcher/
â”‚
â”œâ”€â”€ dspy_impl/               # DSPy: "Define what, not how"
â”‚   â”œâ”€â”€ signatures.py        # Typed input/output contracts
â”‚   â”œâ”€â”€ pipeline.py          # Modules + review loop
â”‚   â””â”€â”€ run.py               # Entry point
â”‚
â”œâ”€â”€ langgraph_impl/          # LangGraph: "Draw your workflow"
â”‚   â”œâ”€â”€ state.py
â”‚   â”œâ”€â”€ nodes.py
â”‚   â”œâ”€â”€ graph.py
â”‚   â””â”€â”€ run.py
â”‚
â””â”€â”€ crewai_impl/             # CrewAI: "Describe your team"
    â”œâ”€â”€ agents.yaml
    â”œâ”€â”€ tasks.yaml
    â”œâ”€â”€ crew.py
    â””â”€â”€ run.py
```

## Quick Start
```bash
git clone https://github.com/aazizisoufiane/dspy-langgraph-crewai-comparison.git
cd dspy-langgraph-crewai-comparison

cp .env.example .env         # add your API key
uv sync                      # install dependencies

# Run any implementation
just dspy "Apple"
just langgraph "Apple"
just crewai "Apple"
just all "Apple"             # run all three
```

## Key Findings (Preview)

| Aspect | DSPy | LangGraph | CrewAI |
|--------|------|-----------|--------|
| Philosophy | Compile & optimize | Engineer & control | Configure & orchestrate |
| MCP integration | Manual wrapper | Ecosystem adapters | Native |
| Skill trigger reliability | Optimizable (GEPA) | Manual prompt tweaking | Manual backstory tweaking |
| LLM-as-a-Judge | First-class module | Manual node + prompt | Agent with reviewer role |
| Prompt optimization | Built-in | None | None |

Full analysis in the [blog series](https://faunaris.ai/blog/dspy-langgraph-crewai-part1).

## Author

**Soufiane Aazizi** â€” Lead AI Engineer | [Faunaris AI](https://faunaris.ai)

12+ years in quantitative finance and AI engineering. Building production LLM systems across pharma, banking, and audit.

## License

MIT